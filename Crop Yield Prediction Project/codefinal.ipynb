{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b9488b-2ef3-4d7f-99b0-fd873b62ccd8",
   "metadata": {},
   "source": [
    "# a-kncn using selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b703e6e1-1be7-425d-897f-4919e06d2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4125114995400184\n"
     ]
    }
   ],
   "source": [
    "#a-kncn using selected features\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Split data into features (independent variables) and target (dependent variable)\n",
    "X = data[['N', 'P', 'pH','Area_in_hectares']]\n",
    "y = data['Soil_Type']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit KMeans clustering to training data to find centroids for each class\n",
    "kmeans = KMeans(n_clusters=len(data['Soil_Type'].unique()),random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(centroids)\n",
    "\n",
    "# Classify test data\n",
    "predicted_labels = []\n",
    "for x in X_test.values:\n",
    "    distances, indices = nbrs.kneighbors([x])\n",
    "    predicted_labels.append(y_train.iloc[indices[0][0]])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adb2bc-add1-41c8-a714-6a15bdd2900c",
   "metadata": {},
   "source": [
    "# akncn-ELM-BOA:(Extreme learning machine-Butterfly optimisation algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75e9ee1-8bed-44b3-b20e-1da1991199ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a-KNCN: 0.4125114995400184\n",
      "Mean Absolute Error: 0.0003578695885072519\n",
      "R2 Score: 0.5010712286426116\n",
      "Median Absolute Error: 0.00014665084704878006\n",
      "Explained Variance Score: 0.5010822368989825\n",
      "Mean Squared Log Error: 7.031190107495536e-05\n",
      "Mean Squared Error: 3.79223679023735e-05\n",
      "Root Mean Squared Error: 0.006158113989069502\n",
      "Mean Absolute Percentage Error: 4.103119425051535\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_error, \n",
    "mean_squared_log_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Extreme Learning Machine (ELM) Implementation\n",
    "class ELMRegressorCustom:\n",
    "    def __init__(self, n_hidden):\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.input_weights = np.random.randn(n_features, self.n_hidden)\n",
    "        H = np.dot(X_train, self.input_weights)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        H = np.dot(X_test, self.input_weights)\n",
    "        y_pred = np.dot(H, self.output_weights)\n",
    "        return y_pred\n",
    "\n",
    "# Butterfly Optimization Algorithm (BOA) Implementation\n",
    "class BOACustom:\n",
    "    def __init__(self, max_iter, pop_size):\n",
    "        self.max_iter = max_iter\n",
    "        self.pop_size = pop_size\n",
    "\n",
    "    def _initialize_population(self, n_features):\n",
    "        return np.random.uniform(-1, 1, size=(self.pop_size, n_features))\n",
    "\n",
    "    def _evaluate_fitness(self, X_train, y_train, population):\n",
    "        fitness = []\n",
    "        for individual in population:\n",
    "            elm = ELMRegressorCustom(n_hidden=len(X_train[0]))\n",
    "            elm.fit(X_train, y_train)\n",
    "            y_pred = elm.predict(X_train)\n",
    "            fitness.append(mean_squared_error(y_train, y_pred))\n",
    "        return np.array(fitness)\n",
    "\n",
    "    def optimize(self, X_train, y_train):\n",
    "        n_features = len(X_train[0])\n",
    "        population = self._initialize_population(n_features)\n",
    "        for _ in range(self.max_iter):\n",
    "            fitness = self._evaluate_fitness(X_train, y_train, population)\n",
    "            sorted_indices = np.argsort(fitness)\n",
    "            best_individual = population[sorted_indices[0]]\n",
    "            new_population = [best_individual]\n",
    "            for i in range(1, self.pop_size):\n",
    "                new_individual = best_individual + np.random.normal(scale=0.1, size=n_features)\n",
    "                new_population.append(new_individual)\n",
    "            population = np.array(new_population)\n",
    "        return {'n_hidden': len(X_train[0])}\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Split data into features (independent variables) and target (dependent variable)\n",
    "X = data[['N', 'P', 'pH', 'Area_in_hectares']]\n",
    "y = data['Soil_Type']\n",
    "\n",
    "# Split data into training and testing sets for a-KNCN\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit KMeans clustering to training data to find centroids for each class\n",
    "kmeans = KMeans(n_clusters=len(data['Soil_Type'].unique()), random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(centroids)\n",
    "\n",
    "# Classify test data using a-KNCN\n",
    "predicted_labels = []\n",
    "for x in X_test.values:\n",
    "    distances, indices = nbrs.kneighbors([x])\n",
    "    predicted_labels.append(y_train.iloc[indices[0][0]])\n",
    "\n",
    "# Calculate accuracy for a-KNCN\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy of a-KNCN:\", accuracy)\n",
    "\n",
    "# Prepare data for ELM\n",
    "X_ELM = data[['Soil_Type', 'rainfall', 'temperature']]\n",
    "y_ELM = data['Yield_ton_per_hec']\n",
    "\n",
    "# Normalize data for ELM\n",
    "scaler = MinMaxScaler()\n",
    "X_ELM_normalized = scaler.fit_transform(X_ELM)\n",
    "\n",
    "# Split data into training and testing sets for ELM\n",
    "X_train_ELM, X_test_ELM, y_train_ELM, y_test_ELM = train_test_split(X_ELM_normalized, y_ELM, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define and optimize ELM model using Butterfly Optimization Algorithm\n",
    "boa = BOACustom(max_iter=100, pop_size=50)\n",
    "best_params = boa.optimize(X_train_ELM, y_train_ELM)\n",
    "\n",
    "# Train ELM model with optimized parameters\n",
    "elm = ELMRegressorCustom(**best_params)\n",
    "elm.fit(X_train_ELM, y_train_ELM)\n",
    "\n",
    "# Predict using trained ELM model\n",
    "y_pred_ELM = elm.predict(X_test_ELM)\n",
    "\n",
    "# Error metrics calculation for ELM\n",
    "mae = mean_absolute_error(y_test_ELM, y_pred_ELM)\n",
    "r2 = r2_score(y_test_ELM, y_pred_ELM)\n",
    "medae = median_absolute_error(y_test_ELM, y_pred_ELM)\n",
    "evs = explained_variance_score(y_test_ELM, y_pred_ELM)\n",
    "msle = mean_squared_log_error(y_test_ELM, y_pred_ELM)\n",
    "mse = mean_squared_error(y_test_ELM, y_pred_ELM)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = np.mean(np.abs((y_test_ELM - y_pred_ELM) / np.clip(np.abs(y_test_ELM), 1e-1, None))) * 100\n",
    "\n",
    "# Error metrics normalization\n",
    "mae_norm = mae / (y_ELM.max() - y_ELM.min())\n",
    "r2_norm = (r2 + 1) / 2  # Ranges from -1 to 1, normalize to 0 to 1\n",
    "medae_norm = medae / (y_ELM.max() - y_ELM.min())\n",
    "evs_norm = (evs + 1) / 2  # Ranges from -1 to 1, normalize to 0 to 1\n",
    "msle_norm = msle / (y_ELM.max() - y_ELM.min())  # Assuming positive target values\n",
    "mse_norm = mse / ((y_ELM.max() - y_ELM.min()) ** 2)\n",
    "rmse_norm = rmse / (y_ELM.max() - y_ELM.min())\n",
    "mape_norm = mape / 100  # Percentage to decimal\n",
    "\n",
    "# Print error metrics\n",
    "print(\"Mean Absolute Error:\", mae_norm)\n",
    "print(\"R2 Score:\", r2_norm)\n",
    "print(\"Median Absolute Error:\", medae_norm)\n",
    "print(\"Explained Variance Score:\", evs_norm)\n",
    "print(\"Mean Squared Log Error:\", msle_norm)\n",
    "print(\"Mean Squared Error:\", mse_norm)\n",
    "print(\"Root Mean Squared Error:\", rmse_norm)\n",
    "print(\"Mean Absolute Percentage Error:\", mape_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffef18-881f-4577-8602-5684cd430a2d",
   "metadata": {},
   "source": [
    "# aKNC-RF(k neighbour classifier-Random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431ece86-934b-46e3-bd02-eddeb924d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8789604998785631\n",
      "Mean Absolute Error (MAE): 0.8101421177498689\n",
      "R-squared: 0.8086717686260186\n",
      "Median Absolute Error (MedAE): 0.3405273030444411\n",
      "Explained Variance Score (EVS): 0.8086717777890843\n",
      "Mean Squared Logarithmic Error (MSLE): 0.06285719753742476\n",
      "Mean Squared Error (MSE): 0.00019974241444261942\n",
      "Root Mean Squared Error (RMSE): 0.014133025664825612\n",
      "Mean Absolute Percentage Error (MAPE): 0.153546500928083\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_log_error, \n",
    "mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for classification\n",
    "X_classification = data.drop('Soil_Type', axis=1)\n",
    "y_classification = data['Soil_Type']\n",
    "\n",
    "# Train the aKNC model for classification\n",
    "aknc = KNeighborsClassifier(n_neighbors=5)\n",
    "aknc.fit(X_classification, y_classification)\n",
    "\n",
    "# Get the class labels from aKNC\n",
    "class_labels = aknc.predict(X_classification)\n",
    "\n",
    "# Add class labels as a feature to the dataset\n",
    "data['class_labels'] = class_labels\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for prediction\n",
    "X_prediction = data.drop('Yield_ton_per_hec', axis=1)\n",
    "y_prediction = data['Yield_ton_per_hec']\n",
    "\n",
    "# Split the data into training and testing sets for prediction\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prediction, y_prediction, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model for prediction\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the combined model\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for classification \n",
    "accuracy = accuracy_score(y_classification, class_labels)\n",
    "\n",
    "# Calculate error metrics for regression\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "medae = median_absolute_error(y_test, predictions)\n",
    "evs = explained_variance_score(y_test, predictions)\n",
    "msle = mean_squared_log_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - predictions) / np.clip(np.abs(y_test), 1e-4, None)))\n",
    "mape /= np.max(y_test)\n",
    "\n",
    "# Normalize error metrics to range [0, 1]\n",
    "max_y = np.max(y_test)\n",
    "mse /= max_y ** 2\n",
    "rmse /= max_y\n",
    "\n",
    "# Print error metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Median Absolute Error (MedAE):\", medae)\n",
    "print(\"Explained Variance Score (EVS):\", evs)\n",
    "print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d179d7f-02e3-4538-a571-8ff3a4a34ebe",
   "metadata": {},
   "source": [
    "# aKNC-GB(k neighbour classifier-gradient boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724006dc-1140-4fc9-a75b-4eb1ae237675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8789604998785631\n",
      "Mean Absolute Error (MAE): 0.39185149864359886\n",
      "R-squared: 0.9706123793957174\n",
      "Median Absolute Error (MedAE): 0.15137676388456955\n",
      "Explained Variance Score (EVS): 0.9706125444671301\n",
      "Mean Squared Error (MSE): 3.068002172010528e-05\n",
      "Root Mean Squared Error (RMSE): 0.005538954930319011\n",
      "Mean Squared Logarithmic Error (MSLE): 0.003098843563190742\n",
      "Mean Absolute Percentage Error (MAPE): 0.08124104156954418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Ensure target variable (Yield_ton_per_hec) is non-negative\n",
    "data['Yield_ton_per_hec'] = data['Yield_ton_per_hec'].clip(lower=0)\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for classification\n",
    "X_classification = data.drop('Soil_Type', axis=1)\n",
    "y_classification = data['Soil_Type']\n",
    "\n",
    "# Train the aKNC model for classification\n",
    "aknc = KNeighborsClassifier(n_neighbors=5)\n",
    "aknc.fit(X_classification, y_classification)\n",
    "\n",
    "# Get the class labels from aKNC\n",
    "class_labels = aknc.predict(X_classification)\n",
    "\n",
    "# Add class labels as a feature to the dataset\n",
    "data['class_labels'] = class_labels\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for prediction\n",
    "X_prediction = data.drop('Yield_ton_per_hec', axis=1)\n",
    "y_prediction = data['Yield_ton_per_hec']\n",
    "\n",
    "# Split the data into training and testing sets for prediction\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prediction, y_prediction, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting model for prediction\n",
    "gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1,max_depth=5,random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the combined model\n",
    "predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "# Calculate accuracy for classification\n",
    "accuracy = accuracy_score(y_classification, class_labels)\n",
    "\n",
    "# Calculate error metrics for regression\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "medae = median_absolute_error(y_test, predictions)\n",
    "evs = explained_variance_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Normalize MSE, RMSE, and MSLE to range [0, 1]\n",
    "max_y = np.max(y_test)\n",
    "mse /= max_y ** 2\n",
    "rmse /= max_y\n",
    "msle = mean_squared_log_error(y_test, predictions)\n",
    "msle_rescaled = np.mean(msle) / np.log(1 + max_y)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test - predictions) / np.clip(np.abs(y_test), 1e-4, None)))\n",
    "mape /= np.max(y_test)\n",
    "\n",
    "# Print accuracy and error metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Median Absolute Error (MedAE):\", medae)\n",
    "print(\"Explained Variance Score (EVS):\", evs)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Squared Logarithmic Error (MSLE):\", msle_rescaled)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26d2fc-7418-4cb5-8cb5-1185e82b8c46",
   "metadata": {},
   "source": [
    "# aKNC-ANN(k neighbour classifier-artificial neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecefabc2-b675-476d-a54f-70650dae641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8789604998785631\n",
      "Mean Absolute Error (MAE): 1.9553142893566406\n",
      "R-squared: 0.5656779228659645\n",
      "Median Absolute Error (MedAE): 0.884876262832373\n",
      "Explained Variance Score (EVS): 0.5726323666455635\n",
      "Mean Squared Error (MSE): 0.000453422580188463\n",
      "Root Mean Squared Error (RMSE): 0.021293721614327146\n",
      "Mean Squared Logarithmic Error (MSLE): 0.06435072044150483\n",
      "Mean Absolute Percentage Error (MAPE): 3.856140969635657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARINY\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_error, \n",
    "mean_squared_log_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for classification\n",
    "X_classification = data.drop('Soil_Type', axis=1)\n",
    "y_classification = data['Soil_Type']\n",
    "\n",
    "# Train the KNC model for classification\n",
    "aknc = KNeighborsClassifier(n_neighbors=5)\n",
    "aknc.fit(X_classification, y_classification)\n",
    "\n",
    "# Get the class labels from aKNC\n",
    "class_labels = aknc.predict(X_classification)\n",
    "\n",
    "# Add class labels as a feature to the dataset\n",
    "data['class_labels'] = class_labels\n",
    "\n",
    "# Split the data into features (X) and target variable (y) for prediction\n",
    "X_prediction = data.drop('Yield_ton_per_hec', axis=1)\n",
    "y_prediction = data['Yield_ton_per_hec']\n",
    "\n",
    "# Split the data into training and testing sets for prediction\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prediction, y_prediction, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features for prediction\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the ANN model for prediction\n",
    "ann_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)\n",
    "ann_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions using the ANN model\n",
    "predictions = ann_model.predict(X_test_scaled)\n",
    "\n",
    "# Ensure predictions and ground truth values are non-negative\n",
    "predictions = np.maximum(predictions, 0)\n",
    "y_test = np.maximum(y_test, 0)\n",
    "\n",
    "# Calculate accuracy for classification\n",
    "accuracy = accuracy_score(y_classification, class_labels)\n",
    "\n",
    "# Calculate error metrics for regression\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "medae = median_absolute_error(y_test, predictions)\n",
    "evs = explained_variance_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "max_y = np.max(y_test)\n",
    "mse /= max_y ** 2\n",
    "rmse /= max_y\n",
    "msle_re = mean_squared_log_error(y_test, predictions)\n",
    "msle = np.mean(msle_re) / np.log(1 + max_y)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test - predictions) / np.clip(np.abs(y_test), 1e-4, None)))\n",
    "mape /= np.max(y_test)\n",
    "\n",
    "# Print accuracy and error metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Median Absolute Error (MedAE):\", medae)\n",
    "print(\"Explained Variance Score (EVS):\", evs)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Squared Logarithmic Error (MSLE):\", msle)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f702c7-885d-45af-af5a-653fbbbea842",
   "metadata": {},
   "source": [
    " # aKNCN-ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd131823-dd40-40be-b93b-151110f36f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4125114995400184\n",
      "Mean Absolute Error: 0.0003357099677737959\n",
      "R^2 Score: 0.004366158967688549\n",
      "Median Absolute Error: 0.00012095560043299153\n",
      "Explained Variance Score: 0.00439412073557599\n",
      "Mean Squared Log Error: 0.00012024393010726097\n",
      "Mean Squared Error: 0.37084885421591673\n",
      "Root Mean Squared Error: 6.213382380968588e-05\n",
      "Mean Absolute Percentage Error: 0.10548734561242336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_log_error, \n",
    "mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class ELMRegressor:\n",
    "    def __init__(self, input_size, hidden_layer_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.input_weights = np.random.rand(input_size, hidden_layer_size)\n",
    "        self.bias = np.random.rand(hidden_layer_size)\n",
    "        self.output_weights = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Calculate hidden layer output\n",
    "        hidden_output = np.dot(X, self.input_weights) + self.bias\n",
    "        hidden_output = self._sigmoid(hidden_output)\n",
    "\n",
    "        # Moore-Penrose pseudo-inverse to calculate output weights\n",
    "        self.output_weights = np.dot(np.linalg.pinv(hidden_output), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_output = np.dot(X, self.input_weights) + self.bias\n",
    "        hidden_output = self._sigmoid(hidden_output)\n",
    "        return np.dot(hidden_output, self.output_weights)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Classification of soil types using KMeans and Nearest Neighbors\n",
    "X_classification = data[['N', 'P', 'pH', 'Area_in_hectares']]\n",
    "y_classification = data['Soil_Type']\n",
    "\n",
    "X_train_classification, X_test_classification, y_train_classification, y_test_classification = train_test_split(X_classification, y_classification, \n",
    "                                                                                                                test_size=0.3, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(data['Soil_Type'].unique()), random_state=42)\n",
    "kmeans.fit(X_train_classification)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=4, algorithm='brute').fit(centroids)\n",
    "\n",
    "predicted_labels = []\n",
    "for x in X_test_classification.values:\n",
    "    distances, indices = nbrs.kneighbors([x])\n",
    "    predicted_labels.append(y_train_classification.iloc[indices[0][0]])\n",
    "\n",
    "accuracy = accuracy_score(y_test_classification, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Predicting crop yield using Extreme Learning Machine (ELM)\n",
    "X_regression = data[['Soil_Type', 'rainfall', 'temperature']]\n",
    "y_regression = data['Yield_ton_per_hec']\n",
    "\n",
    "X_train_regression, X_test_regression, y_train_regression, y_test_regression = train_test_split(X_regression, y_regression, test_size=0.3, \n",
    "                                                                                                random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_regression_scaled = scaler.fit_transform(X_train_regression)\n",
    "X_test_regression_scaled = scaler.transform(X_test_regression)\n",
    "\n",
    "# Train ELM model\n",
    "elm = ELMRegressor(input_size=X_train_regression_scaled.shape[1], hidden_layer_size=80)  # Adjust hidden_layer_size as needed\n",
    "elm.train(X_train_regression_scaled, y_train_regression)\n",
    "\n",
    "# Predict crop yield\n",
    "y_pred_regression = elm.predict(X_test_regression_scaled)\n",
    "\n",
    "# Transform target values to ensure they are non-negative\n",
    "y_pred_regression_transformed = y_pred_regression - y_pred_regression.min() + 1\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(y_test_regression, y_pred_regression) / np.max(y_test_regression)\n",
    "r2 = r2_score(y_test_regression, y_pred_regression)\n",
    "medae = median_absolute_error(y_test_regression, y_pred_regression) / np.max(y_test_regression)\n",
    "evs = explained_variance_score(y_test_regression, y_pred_regression)\n",
    "msle = mean_squared_log_error(y_test_regression, y_pred_regression_transformed) / np.max(y_test_regression)\n",
    "mse = mean_squared_error(y_test_regression, y_pred_regression) / np.max(y_test_regression)\n",
    "rmse = np.sqrt(mse) / np.max(y_test_regression)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test_regression - y_pred_regression) / np.clip(np.abs(y_test_regression), 1e-4, None)))\n",
    "mape = mape / np.max(y_test_regression)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Median Absolute Error:\", medae)\n",
    "print(\"Explained Variance Score:\", evs)\n",
    "print(\"Mean Squared Log Error:\", msle)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac6074-200f-4def-8a1d-fa5808e3dfb3",
   "metadata": {},
   "source": [
    " # aKNC-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b06a260-c81a-4bfe-8ac2-e44414f246e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-Nearest Neighbors Classifier: 0.7274383176022521\n",
      "\n",
      "Error Metrics:\n",
      "Mean Absolute Error: 0.559131837080325\n",
      "R2 Score: 0.44622379555260683\n",
      "Median Absolute Error: 0.28\n",
      "Explained Variance Score: 0.4464382441401469\n",
      "Mean Squared Log Error: 0.208065281009776\n",
      "Mean Squared Error: 0.7793203338602849\n",
      "Root Mean Squared Error: 0.88279121759354\n",
      "Mean Absolute Percentage Error: 0.005591318370803251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_log_error, \n",
    "mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"crt_data.csv\")  # Assuming you have a CSV file named \"crt_data.csv\"\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data[['N', 'P', 'pH', 'Area_in_hectares']]\n",
    "y = data['Soil_Type']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train k-Nearest Neighbors Classifier\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict Soil_Type for test set\n",
    "y_pred = knc.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of k-Nearest Neighbors Classifier:\", accuracy)\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=2)  # Choose appropriate number of components\n",
    "X_reg_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split regression data into train and test sets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predict Soil_Type for test set\n",
    "y_reg_pred = rf_reg.predict(X_reg_test)\n",
    "\n",
    "# Error metrics\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "medianae = median_absolute_error(y_reg_test, y_reg_pred)\n",
    "evs = explained_variance_score(y_reg_test, y_reg_pred)\n",
    "msle = mean_squared_log_error(y_reg_test, y_reg_pred)\n",
    "mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_reg_test - y_reg_pred) / np.clip(np.abs(y_reg_test),1e2, None)))\n",
    "\n",
    "# Print error metrics\n",
    "print(\"\\nError Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R2 Score:\", r2)\n",
    "print(\"Median Absolute Error:\", medianae)\n",
    "print(\"Explained Variance Score:\", evs)\n",
    "print(\"Mean Squared Log Error:\", msle)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593fdb2-333f-4933-8dc1-d0a039d147ee",
   "metadata": {},
   "source": [
    "# aKNCN-ELM-mBOA(modified Butterfly Optimisation Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777db3f9-ee89-409f-a3be-ba37bc7bc38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a-KNCN: 0.4125114995400184\n",
      "Mean Absolute Error: 0.00035782988832440627\n",
      "R2 Score: 0.5010707370734182\n",
      "Median Absolute Error: 0.00014664929233942965\n",
      "Explained Variance Score: 0.5010818628134308\n",
      "Mean Squared Log Error: 7.030317421469163e-05\n",
      "Mean Squared Error: 3.7922405265357684e-05\n",
      "Root Mean Squared Error: 0.0061581170227073215\n",
      "Mean Absolute Percentage Error: 1.473432988635159\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_error,\n",
    "mean_squared_log_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "# Extreme Learning Machine (ELM) Implementation\n",
    "class ELMRegressorCustom:\n",
    "    def __init__(self, n_hidden):\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.input_weights = np.random.randn(n_features, self.n_hidden)\n",
    "        H = np.dot(X_train, self.input_weights)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        H = np.dot(X_test, self.input_weights)\n",
    "        y_pred = np.dot(H, self.output_weights)\n",
    "        return y_pred\n",
    "\n",
    "# Modified Butterfly Optimization Algorithm (MBOA) Implementation\n",
    "class MBOACustom:\n",
    "    def __init__(self, max_iter, pop_size, weighing_constant):\n",
    "        self.max_iter = max_iter\n",
    "        self.pop_size = pop_size\n",
    "        self.weighing_constant = weighing_constant\n",
    "\n",
    "    def _initialize_population(self, n_features):\n",
    "        return np.random.uniform(-1, 1, size=(self.pop_size, n_features))\n",
    "\n",
    "    def _evaluate_fitness(self, X_train, y_train, population, n_hidden):\n",
    "        fitness = []\n",
    "        for individual in population:\n",
    "            elm = ELMRegressorCustom(n_hidden=n_hidden)\n",
    "            elm.fit(X_train, y_train)\n",
    "            y_pred = elm.predict(X_train)\n",
    "            fitness.append(mean_squared_error(y_train, y_pred))\n",
    "        return np.array(fitness)\n",
    "\n",
    "    def _local_search(self, population):\n",
    "        mutated_population = []\n",
    "        for individual in population:\n",
    "            new_individual = individual + np.random.normal(scale=self.weighing_constant, size=len(individual))\n",
    "            mutated_population.append(new_individual)\n",
    "        return np.array(mutated_population)\n",
    "\n",
    "    def optimize(self, X_train, y_train, n_hidden):\n",
    "        n_features = len(X_train[0])\n",
    "        population = self._initialize_population(n_features)\n",
    "        for _ in range(self.max_iter):\n",
    "            fitness = self._evaluate_fitness(X_train, y_train, population, n_hidden)\n",
    "            sorted_indices = np.argsort(fitness)\n",
    "            best_individual = population[sorted_indices[0]]\n",
    "            new_population = [best_individual]\n",
    "            for i in range(1, self.pop_size):\n",
    "                new_individual = best_individual + np.random.normal(scale=0.1, size=n_features)\n",
    "                new_population.append(new_individual)\n",
    "            local_population = self._local_search(population)\n",
    "            population = np.concatenate((np.array(new_population), local_population))\n",
    "        return {'n_hidden': n_hidden}\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"crt_data.csv\")\n",
    "\n",
    "# Split data into features (independent variables) and target (dependent variable)\n",
    "X = data[['N', 'P', 'pH', 'Area_in_hectares']]\n",
    "y = data['Soil_Type']\n",
    "\n",
    "# Split data into training and testing sets for a-KNCN\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit KMeans clustering to training data to find centroids for each class\n",
    "kmeans = KMeans(n_clusters=len(data['Soil_Type'].unique()), random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(centroids)\n",
    "\n",
    "# Classify test data using a-KNCN\n",
    "predicted_labels = []\n",
    "for x in X_test.values:\n",
    "    distances, indices = nbrs.kneighbors([x])\n",
    "    predicted_labels.append(y_train.iloc[indices[0][0]])\n",
    "\n",
    "# Calculate accuracy for a-KNCN\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy of a-KNCN:\", accuracy)\n",
    "\n",
    "# Prepare data for ELM\n",
    "X_ELM = data[['Soil_Type', 'rainfall', 'temperature']]\n",
    "y_ELM = data['Yield_ton_per_hec']\n",
    "\n",
    "# Normalize data for ELM\n",
    "scaler = MinMaxScaler()\n",
    "X_ELM_normalized = scaler.fit_transform(X_ELM)\n",
    "\n",
    "# Split data into training and testing sets for ELM\n",
    "X_train_ELM, X_test_ELM, y_train_ELM, y_test_ELM = train_test_split(X_ELM_normalized, y_ELM, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define and optimize ELM model using Modified Butterfly Optimization Algorithm\n",
    "start_time = time.time()\n",
    "mboa = MBOACustom(max_iter=10, pop_size=10, weighing_constant=0.01)  # Adjusted parameters\n",
    "best_params = mboa.optimize(X_train_ELM, y_train_ELM, n_hidden=10)\n",
    "\n",
    "# Train ELM model with optimized parameters\n",
    "elm = ELMRegressorCustom(**best_params)\n",
    "elm.fit(X_train_ELM, y_train_ELM)\n",
    "\n",
    "# Predict using trained ELM model\n",
    "y_pred_ELM = elm.predict(X_test_ELM)\n",
    "\n",
    "# Error metrics calculation for ELM\n",
    "mae = mean_absolute_error(y_test_ELM, y_pred_ELM)\n",
    "r2 = r2_score(y_test_ELM, y_pred_ELM)\n",
    "medae = median_absolute_error(y_test_ELM, y_pred_ELM)\n",
    "evs = explained_variance_score(y_test_ELM, y_pred_ELM)\n",
    "msle = mean_squared_log_error(y_test_ELM, y_pred_ELM)\n",
    "mse = mean_squared_error(y_test_ELM, y_pred_ELM)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = np.mean(np.abs((y_test_ELM - y_pred_ELM) / np.maximum(np.abs(y_test_ELM), 1))) * 100\n",
    "\n",
    "# Error metrics normalization\n",
    "mae_norm = mae / (y_ELM.max() - y_ELM.min())\n",
    "r2_norm = (r2 + 1) / 2  # Ranges from -1 to 1, normalize to 0 to 1\n",
    "medae_norm = medae / (y_ELM.max() - y_ELM.min())\n",
    "evs_norm = (evs + 1) / 2  # Ranges from -1 to 1, normalize to 0 to 1\n",
    "msle_norm = msle / (y_ELM.max() - y_ELM.min())  # Assuming positive target values\n",
    "mse_norm = mse / ((y_ELM.max() - y_ELM.min()) ** 2)\n",
    "rmse_norm = rmse / (y_ELM.max() - y_ELM.min())\n",
    "mape_norm = mape / 100  # Percentage to decimal\n",
    "\n",
    "# Print error metrics\n",
    "print(\"Mean Absolute Error:\", mae_norm)\n",
    "print(\"R2 Score:\", r2_norm)\n",
    "print(\"Median Absolute Error:\", medae_norm)\n",
    "print(\"Explained Variance Score:\", evs_norm)\n",
    "print(\"Mean Squared Log Error:\", msle_norm)\n",
    "print(\"Mean Squared Error:\", mse_norm)\n",
    "print(\"Root Mean Squared Error:\", rmse_norm)\n",
    "print(\"Mean Absolute Percentage Error:\", mape_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78aed1c-a8e4-454b-b085-1da505e5d14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
